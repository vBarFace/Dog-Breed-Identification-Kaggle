{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5baa754f-a4bf-40ee-965e-d888cccf1df7",
   "metadata": {},
   "source": [
    "# Dog Breed Identification: Machine Learning from Kaggle Competition\n",
    "\n",
    "My name is André Fernandes and in this notebook is presented my solution proposal for the competition. Feel free to connect with me on LinkedIn and check out my other projects on GitHub:\n",
    "\n",
    "[LinkedIn](https://www.linkedin.com)\n",
    "[GitHub](https://www.linkedin.com/in/andr%C3%A9-fernandes-868006207/)\n",
    "\n",
    "Below is the description of the competition and the link to the main page if you want to check it for yourself.\n",
    "\n",
    "**Competition Description**\n",
    "\n",
    "The Dog Breed Identification competition challenges participants to identify the breed of a dog in an image. This notebook will guide you through the process of building a predictive model that classifies dog breeds based on image data.\n",
    "\n",
    "This competition is hosted on Kaggle: [Dog Breed Identification](https://www.kaggle.com/competitions/dog-breed-identification/overview)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Introduction\n",
    "2. Data Description\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Data Preprocessing\n",
    "5. Modeling\n",
    "6. Model Evaluation\n",
    "7. Conclusion\n",
    "8. Usage\n",
    "9. References\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Dog Breed Identification dataset is a collection of dog images categorized into various breeds. This notebook aims to develop a model that accurately classifies dog breeds based on image data.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The dataset consists of three main components:\n",
    "\n",
    "- **train.zip**: The training dataset containing images of dogs and their corresponding breed labels.\n",
    "- **test.zip**: The test dataset for which predictions need to be made.\n",
    "- **labels.csv**: A CSV file containing the breed labels for the training dataset images.\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "...\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "...\n",
    "\n",
    "## Modeling\n",
    "\n",
    "...\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "....\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "...\n",
    "\n",
    "## Usage\n",
    "\n",
    "...\n",
    "\n",
    "## References\n",
    "\n",
    "- Kaggle Dog Breed Identification Competition: [Kaggle Dog Breed Identification](https://www.kaggle.com/competitions/dog-breed-identification/overview)\n",
    "- ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575573db-a7ce-407c-bd7f-2ae317cf66b7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6204f4-2c0d-400b-b4f1-234a3723b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulate data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Handleing files\n",
    "import os\n",
    "from PIL import Image\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Encode the labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Process data before modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# For modeling\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# For model evaluation\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286a706-3d3b-414e-b759-8d2e644e6670",
   "metadata": {},
   "source": [
    "# Global constants/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ab228-9654-43dc-9b00-db9dcd2406ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "N_SAMPLES = 100 # For EDA (excluding IMG samples)\n",
    "IMG_SIZE = 224 # This is popular size to performe image resizing\n",
    "BATCH_SIZE = 32 # This is a usual batch size to use\n",
    "N_CHANNELS = 3 # Number of channels of image, RGB\n",
    "N_EPOCHES = 25 # Number of times the training process will run though the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf62c53-5252-4af1-b3ce-49b523586478",
   "metadata": {},
   "source": [
    "# Working/Processing Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ea0fd-f929-4df6-9811-18e7b1af0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels\n",
    "labels = pd.read_csv('../data/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a7d44-d77e-41d7-8b9e-1dde34b4c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see labels info\n",
    "labels.info()\n",
    "\n",
    "# From here we can conclude that there are 10222 labels, that is, 10222 samples.\n",
    "# Also, there are not Null values and that the labels are object type so they will need to be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660af11-ab0c-4c04-9945-231ab7e0ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the first five elements of the labels dataframe\n",
    "labels.head()\n",
    "\n",
    "# We can see that there are 2 collumns.\n",
    "# The 'id' represents a unique representation of each image\n",
    "# The 'breed' represents tha breed assign to each dog in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb554e1-a771-4a19-9e80-3de2ebb35348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets add a new collumn to the labels, the image path\n",
    "labels['img_path'] = labels['id'].apply(lambda x: os.path.join('../data/train', f'{x}.jpg'))\n",
    "labels.info()\n",
    "\n",
    "# We can confirm once again that there are no Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d79f3-21cd-45ec-958c-147fdf446f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the first five elements of the new labels dataframe\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65a50d-2ddd-4ff5-8cf8-f2d5c8ec36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets extract the number of breeds there are in the dataset\n",
    "n_breeds = labels['breed'].nunique()\n",
    "print(f\"There are {n_breeds} unique dog breeds.\")\n",
    "\n",
    "# Create a seaboarn histogram to study the distribution of the breeds\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.countplot(data=labels, x='breed')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Breed')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Count of Each Breed in the DataFrame')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# From this histogram we can conclude that there is no anomaly distribution in the number of each breed\n",
    "# Beeing so, we can then encode the breed, the thing we are aiming to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be036d-0fd2-4cea-988a-3c4976ff9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# Lets create a new collumn in the labels dataframe that representes the encoded breed\n",
    "labels['breed_encoded'] = label_encoder.fit_transform(labels['breed'])\n",
    "\n",
    "# Lets see the information\n",
    "labels.info()\n",
    "\n",
    "# So, for the first try we will be using the LabelEncoder()\n",
    "# In the info, we can see that the type of the encodation is integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae319cb9-d5a7-46ba-87b9-94c2f6565771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the first five elements of the labels dataframe now, with the encoded breeds\n",
    "labels.head()\n",
    "\n",
    "# From here, we can conclude that each breed is now associeated with one number.\n",
    "# This can possible lead our modle to give more importance to the breeds encoded with higher numbers so we have to have that in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a0b9c-2393-44a4-95d0-eebb3d984496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can just confirm if there are no Null values\n",
    "print(\"Number of Null values in each collumn of labels dataframe:\\n\")\n",
    "print(labels.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e50c76-674a-43c7-a16d-7fcff6076ff6",
   "metadata": {},
   "source": [
    "# Perform some EDA on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b977aa-84b2-43ed-b2f7-265dfa5e526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_images(labels_df, num_samples=5):\n",
    "    '''\n",
    "    Goal: display sample images from the dataframe\n",
    "\n",
    "    Inputs: -> the dataframe\n",
    "            -> the number of samples to display\n",
    "    '''\n",
    "    breeds = labels_df['breed'].unique()\n",
    "    sample_images = []\n",
    "\n",
    "    for breed in breeds[:num_samples]:\n",
    "        # We filter the dataframe with only the breed 'breed' and then randomly sample 1 from it\n",
    "        sample_image_path = labels_df[labels_df['breed'] == breed].sample(1)['img_path'].values[0]\n",
    "        # Then we save the tuple in the array\n",
    "        sample_images.append((breed, sample_image_path))\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (breed, image_path) in enumerate(sample_images):\n",
    "        image = Image.open(image_path)\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(breed)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6277c6a-f133-459a-b09d-2be7ba8d6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see some random images from the dataframe\n",
    "display_sample_images(labels, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8d9e8-b65f-4ba2-9a41-908d51ee5374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image dimensions\n",
    "image_shapes = []\n",
    "for img_path in labels['img_path'].sample(N_SAMPLES):  # Sample for speed\n",
    "    with Image.open(img_path) as img:\n",
    "        image_shapes.append(img.size)\n",
    "\n",
    "# Create a dataframe of the images shape\n",
    "image_shapes_df = pd.DataFrame(image_shapes, columns=['Width', 'Height'])\n",
    "# Lets print it\n",
    "print(\"Images Dimensions Imformation:\\n\")\n",
    "print(image_shapes_df.describe()) # We can use the mean Width and mean Height to finetune IMG_SIZE to resize in the future\n",
    "\n",
    "# Plot image dimensions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(image_shapes_df['Width'], bins=30, color='blue', label='Width', kde=True)\n",
    "sns.histplot(image_shapes_df['Height'], bins=30, color='red', label='Height', kde=True)\n",
    "plt.title('Distribution of Images Dimensions')\n",
    "plt.xlabel('Pixels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667c603-633b-4373-bfdf-40b3b83b5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pixel_intensity(image_path):\n",
    "    '''\n",
    "    Goal: analyze image pixel intensity distribution of one image\n",
    "\n",
    "    Input: the path of an image\n",
    "\n",
    "    Outputs: -> The mean intensity of pixels in the image\n",
    "             -> The standard diviation of the pixels in the image\n",
    "    '''\n",
    "    image = Image.open(image_path)\n",
    "    image_array = np.array(image)\n",
    "    return image_array.mean(), image_array.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e0f80-f614-4fba-889f-b2fd8a87e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get analize the pixel_intensity of 100 random images\n",
    "pixel_intensity_stats = [analyze_pixel_intensity(img_path) for img_path in labels['img_path'].sample(N_SAMPLES)]\n",
    "# Lets create a dataframe for it\n",
    "pixel_intensity_df = pd.DataFrame(pixel_intensity_stats, columns=['Mean Intensity', 'Intensity Std'])\n",
    "# Lets print it\n",
    "print(pixel_intensity_df.describe())\n",
    "\n",
    "# Plot the distribution of image mean pixel intensity\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(pixel_intensity_df['Mean Intensity'], bins=30, color='blue', kde=True)\n",
    "plt.title('Distribution of Image Mean Pixel Intensity')\n",
    "plt.xlabel('Mean Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of image pixel intensity standard deviation\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(pixel_intensity_df['Intensity Std'], bins=30, color='red', kde=True)\n",
    "plt.title('Distribution of Image Intensity Standard Deviation')\n",
    "plt.xlabel('Intensity Std')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# As we can see, both look like that their folling a normal distribuition which is postive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea6554-bbee-4d47-bf27-18694b51c434",
   "metadata": {},
   "source": [
    "# Working/Processing Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12512241-eaff-4ca3-b2b6-31b4e0ebfe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_df, val_df = train_test_split(labels, test_size=0.2, stratify=labels['breed_encoded'], random_state=42)\n",
    "\n",
    "# First, we'll use the labels dataframe to split the dataset in training and validation.\n",
    "# We can do this beacuse we have the images path in the labels dataframe.\n",
    "# We will use a 20% split, that is, 80% of the data will be used to training and 20% for validation\n",
    "# The random state allows us to test always in the same data split that is, all same images go to train and alll same images go to validation\n",
    "#no matter the times we runn the code\n",
    "# We use stratify in realtion to labels_df['breed_encoded'] to make a split so that the proportion of values in the sample produced will \n",
    "#be the same as the proportion of values. Ecample. If 25% of the data is 0's and 75% is 1's, each split will have 25% 0's and 75% 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d893fed-73f2-4a53-b98b-80daf4516719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, label):\n",
    "    '''\n",
    "    Goal of function: pre-process images for training\n",
    "    \n",
    "    Inputs: -> image_path in the disk\n",
    "            -> the label associated with the image\n",
    "            \n",
    "    Outputs: -> the image pre-processed\n",
    "             -> the corresponding label of the pre-processed image\n",
    "    '''\n",
    "    \n",
    "    # Read the image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Decode the image\n",
    "    image = tf.image.decode_jpeg(image, channels=N_CHANNELS)\n",
    "    # Resize the image\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    # Scale the image to [0, 1]\n",
    "    image = image / 255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9835941-6080-4d40-932c-0a536312975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, label):\n",
    "    '''\n",
    "    Goal of function: Apply data augmentation so our training set hcan help \n",
    "                     improve the robustness and generalization of your model by creating variations of the training data.\n",
    "\n",
    "    Inputs: -> an image\n",
    "            -> the image label\n",
    "\n",
    "    Output: -> an image where all augmentation were appplyd\n",
    "            -> the image label\n",
    "    '''\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    image = tf.image.random_saturation(image, lower=0.9, upper=1.1)\n",
    "    return image, label\n",
    "\n",
    "# NOTE: The transformations (e.g., flips, brightness adjustments) are applied randomly each time an image is fetched from the dataset. \n",
    "#      Therefore, every epoch could potentially see a slightly different version of the same image.\n",
    "# It is good because: -> there is no additional storage cost.\n",
    "#                     ->  Each epoch, and even each batch, may see different augmentations, effectively increasing the diversity of the training data.\n",
    "#                     -> Models trained with augmented data are generally more robust and better at generalizing to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c231c7e-06f5-47a5-98d6-667db867f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets for the training dataframe; from_tensor_slices to convert from pandas dataframe to tf dataset.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['img_path'], train_df['breed_encoded']))\n",
    "\n",
    "# We are using TensorFlow datasets instead of Pandas dataframe beacause:\n",
    "# Performance and Efficiency:\n",
    "# -> Efficient Data Loading: TensorFlow Datasets can efficiently load, preprocess, and batch the data on the fly. \n",
    "#                            This reduces memory usage and leverages parallelism, ensuring that data is fed to the model quickly without \n",
    "#                            being bottlenecked by I/O operations.\n",
    "# -> Prefetching: TensorFlow datasets support prefetching, which allows data to be prepared while the model is training. \n",
    "#                 This keeps the GPU/TPU utilized, minimizing idle times.\n",
    "\n",
    "# Integration with TensorFlow/Keras:\n",
    "# -> Seamless Integration: TensorFlow Datasets are designed to work seamlessly with TensorFlow and Keras. \n",
    "#                          They fit naturally into the model.fit method, supporting features like shuffling, batching, and preprocessing.\n",
    "\n",
    "# Scalability:\n",
    "# -> Handling Large Datasets: For large datasets that don’t fit into memory, TensorFlow Datasets can handle data \n",
    "#                            from multiple sources (e.g., files, databases) efficiently without loading everything into memory at once.\n",
    "    \n",
    "# Data Augmentation:\n",
    "# -> On-the-fly Augmentation: TensorFlow datasets support on-the-fly data augmentation, which can be crucial for training \n",
    "#                            robust models without needing to store augmented data on disk.\n",
    "\n",
    "# Lets see the first 5 lines\n",
    "count = 0\n",
    "for img, enc_label in train_ds:\n",
    "    print(img, enc_label)\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5f8ef-cb8d-4d24-b4ce-deacac8a5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the tf dataset we can pre-process\n",
    "train_ds = train_ds.map(preprocess_image).map(augment_image).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Here, the .map applies a function to each element of the dataset. It appplies the preprocess_image fuction to every tuple\n",
    "# After the image is pre-processed we create \n",
    "# The .batch groups consecutive elements of the dataset into batches of size BATCH_SIZE. Each batch will contain (img, label) * BATCH_SIZE pairs.\n",
    "# Finaly, .prefetch ensures that while one batch of data is being processed by the model, the next batch is being prepared, reducing idle time.\n",
    "# The AUTOTUNE option automatically tunes the prefetch buffer size for optimal performance.\n",
    "\n",
    "# Lets see an example\n",
    "print(next(iter(train_ds))) # The output is a batch, that is, there are 32 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070de771-ddf1-4f04-8969-e67a0e929086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we make the exact same thing but on the validation dataframe, that is, we convert it to ts Dataset\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_df['img_path'], val_df['breed_encoded']))\n",
    "val_ds = val_ds.map(preprocess_image).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94c414-49f2-489a-ab79-3f3c84fadcf4",
   "metadata": {},
   "source": [
    "# Modeling - Using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d394ce-0eff-45b0-b678-eae0616efd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first be using the VGG16 by the Tranfer Learning mode\n",
    "VGG16_model = VGG16(input_shape=(IMG_SIZE, IMG_SIZE, N_CHANNELS), include_top=False, weights='imagenet')\n",
    "\n",
    "# Here we import the VGG16 model. This is a CNN considered to be one of the best computer vision models to date.\n",
    "#    - https://medium.com/@mygreatlearning/everything-you-need-to-know-about-vgg16-7315defb5918\n",
    "\n",
    "# When we use \"include_top=False\" we are excluding the fully connected layers and \"weights='imagenet'\" the model is pre-training on ImageNet.\n",
    "# ImageNet is an image database - https://www.image-net.org/\n",
    "# For more parameters: https://keras.io/api/applications/vgg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7598d-6af4-44dc-8ae8-4518e3c608fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets say to the model that we dont want to train the layers of VGG16 because they are alredy pre-trained\n",
    "for layer in VGG16_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb2c9f-117b-490f-a25a-54c1b1f7c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets define the fully connected part of the model\n",
    "first_model = Sequential([\n",
    "    VGG16_model,\n",
    "    Flatten(), # We flatten to prepare the output\n",
    "    Dense(256, activation='relu'), # We define a fully connected layer with 256 nodes and 'relu' activation function\n",
    "    Dropout(0.5), # We prevent overfitting by randomly define 50% of the input units will be randomly set to zero at each update during training time\n",
    "    Dense(n_breeds, activation='softmax') # Then we define the output with 'softmax' because we want a probabiliti for each breed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377ffc1-e0db-4d9f-bedb-5147336ae1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets compile the model\n",
    "first_model.compile(\n",
    "    optimizer='adam', # Lets use 'adam' as the optimizer\n",
    "    loss='sparse_categorical_crossentropy', # categorical_crossentropy is used when labels are encoded with ONE HOT ENCODE\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a989213-e2b5-41b7-b22d-af911d4d3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finaly we train the model\n",
    "history = first_model.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds,\n",
    "    epochs= N_EPOCHES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043ec8c-95b4-4509-9ad8-b6172e9fb1b9",
   "metadata": {},
   "source": [
    "# Evaluation - VGG16 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910901a-f42f-4178-b42e-a74d56b7f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Training and Validation Accuracy\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "ax[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax[0].set_title('Model Accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plotting Training and Validation Loss\n",
    "ax[1].plot(history.history['loss'], label='Train Loss')\n",
    "ax[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax[1].set_title('Model Loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d486049-fa74-4065-bf65-16a44d0bcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "val_ds = val_ds.unbatch()\n",
    "y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "val_ds = val_ds.batch(BATCH_SIZE)\n",
    "y_pred = model.predict(val_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
