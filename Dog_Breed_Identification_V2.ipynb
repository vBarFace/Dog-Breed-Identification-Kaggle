{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5baa754f-a4bf-40ee-965e-d888cccf1df7",
   "metadata": {},
   "source": [
    "# Dog Breed Identification: Machine Learning from Kaggle Competition\n",
    "\n",
    "My name is AndrÃ© Fernandes and in this notebook is presented my solution proposal for the competition. Feel free to connect with me on LinkedIn and check out my other projects on GitHub:\n",
    "\n",
    "[LinkedIn](https://www.linkedin.com)\n",
    "[GitHub](https://www.linkedin.com/in/andr%C3%A9-fernandes-868006207/)\n",
    "\n",
    "Below is the description of the competition and the link to the main page if you want to check it for yourself.\n",
    "\n",
    "**Competition Description**\n",
    "\n",
    "The Dog Breed Identification competition challenges participants to identify the breed of a dog in an image. This notebook will guide you through the process of building a predictive model that classifies dog breeds based on image data.\n",
    "\n",
    "This competition is hosted on Kaggle: [Dog Breed Identification](https://www.kaggle.com/competitions/dog-breed-identification/overview)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Introduction\n",
    "2. Data Description\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Data Preprocessing\n",
    "5. Modeling\n",
    "6. Model Evaluation\n",
    "7. Conclusion\n",
    "8. Usage\n",
    "9. References\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Dog Breed Identification dataset is a collection of dog images categorized into various breeds. This notebook aims to develop a model that accurately classifies dog breeds based on image data.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The dataset consists of three main components:\n",
    "\n",
    "- **train.zip**: The training dataset containing images of dogs and their corresponding breed labels.\n",
    "- **test.zip**: The test dataset for which predictions need to be made.\n",
    "- **labels.csv**: A CSV file containing the breed labels for the training dataset images.\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "...\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "...\n",
    "\n",
    "## Modeling\n",
    "\n",
    "...\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "....\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "...\n",
    "\n",
    "## Usage\n",
    "\n",
    "...\n",
    "\n",
    "## References\n",
    "\n",
    "- Kaggle Dog Breed Identification Competition: [Kaggle Dog Breed Identification](https://www.kaggle.com/competitions/dog-breed-identification/overview)\n",
    "- ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575573db-a7ce-407c-bd7f-2ae317cf66b7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6204f4-2c0d-400b-b4f1-234a3723b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulate data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Handleing files\n",
    "import os\n",
    "from PIL import Image\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Encode the labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Process data before modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# For modeling\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# For model evaluation\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286a706-3d3b-414e-b759-8d2e644e6670",
   "metadata": {},
   "source": [
    "# Global constants/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ab228-9654-43dc-9b00-db9dcd2406ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "N_SAMPLES = 100 # For EDA (excluding IMG samples)\n",
    "IMG_SIZE = 224 # This is popular size to performe image resizing\n",
    "BATCH_SIZE = 32 # This is a usual batch size to use\n",
    "N_CHANNELS = 3 # Number of channels of image, RGB\n",
    "N_EPOCHES = 25 # Number of times the training process will run though the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf62c53-5252-4af1-b3ce-49b523586478",
   "metadata": {},
   "source": [
    "# Working/Processing Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ea0fd-f929-4df6-9811-18e7b1af0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels\n",
    "labels = pd.read_csv('../data/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a7d44-d77e-41d7-8b9e-1dde34b4c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see labels info\n",
    "labels.info()\n",
    "\n",
    "# From here we can conclude that there are 10222 labels, that is, 10222 samples.\n",
    "# Also, there are not Null values and that the labels are object type so they will need to be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660af11-ab0c-4c04-9945-231ab7e0ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the first five elements of the labels dataframe\n",
    "labels.head()\n",
    "\n",
    "# We can see that there are 2 collumns.\n",
    "# The 'id' represents a unique representation of each image\n",
    "# The 'breed' represents tha breed assign to each dog in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb554e1-a771-4a19-9e80-3de2ebb35348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets add a new collumn to the labels, the image path\n",
    "labels['img_path'] = labels['id'].apply(lambda x: os.path.join('../data/train', f'{x}.jpg'))\n",
    "labels.info()\n",
    "\n",
    "# We can confirm once again that there are no Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d79f3-21cd-45ec-958c-147fdf446f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the first five elements of the new labels dataframe\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65a50d-2ddd-4ff5-8cf8-f2d5c8ec36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets extract the number of breeds there are in the dataset\n",
    "n_breeds = labels['breed'].nunique()\n",
    "print(f\"There are {n_breeds} unique dog breeds.\")\n",
    "\n",
    "# Create a seaboarn histogram to study the distribution of the breeds\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.countplot(data=labels, x='breed')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Breed')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Count of Each Breed in the DataFrame')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# From this histogram we can conclude that there is no anomaly distribution in the number of each breed\n",
    "# Beeing so, we can then encode the breed, the thing we are aiming to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be036d-0fd2-4cea-988a-3c4976ff9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# Lets create a new collumn in the labels dataframe that representes the encoded breed\n",
    "labels['breed_encoded'] = label_encoder.fit_transform(labels['breed'])\n",
    "\n",
    "# Lets see the information\n",
    "labels.info()\n",
    "\n",
    "# So, for the first try we will be using the LabelEncoder()\n",
    "# In the info, we can see that the type of the encodation is integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae319cb9-d5a7-46ba-87b9-94c2f6565771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the first five elements of the labels dataframe now, with the encoded breeds\n",
    "labels.head()\n",
    "\n",
    "# From here, we can conclude that each breed is now associeated with one number.\n",
    "# This can possible lead our modle to give more importance to the breeds encoded with higher numbers so we have to have that in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a0b9c-2393-44a4-95d0-eebb3d984496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can just confirm if there are no Null values\n",
    "print(\"Number of Null values in each collumn of labels dataframe:\\n\")\n",
    "print(labels.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e50c76-674a-43c7-a16d-7fcff6076ff6",
   "metadata": {},
   "source": [
    "# Perform some EDA on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b977aa-84b2-43ed-b2f7-265dfa5e526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_images(labels_df, num_samples=5):\n",
    "    '''\n",
    "    Goal: display sample images from the dataframe\n",
    "\n",
    "    Inputs: -> the dataframe\n",
    "            -> the number of samples to display\n",
    "    '''\n",
    "    breeds = labels_df['breed'].unique()\n",
    "    sample_images = []\n",
    "\n",
    "    for breed in breeds[:num_samples]:\n",
    "        # We filter the dataframe with only the breed 'breed' and then randomly sample 1 from it\n",
    "        sample_image_path = labels_df[labels_df['breed'] == breed].sample(1)['img_path'].values[0]\n",
    "        # Then we save the tuple in the array\n",
    "        sample_images.append((breed, sample_image_path))\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (breed, image_path) in enumerate(sample_images):\n",
    "        image = Image.open(image_path)\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(breed)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6277c6a-f133-459a-b09d-2be7ba8d6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see some random images from the dataframe\n",
    "display_sample_images(labels, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8d9e8-b65f-4ba2-9a41-908d51ee5374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image dimensions\n",
    "image_shapes = []\n",
    "for img_path in labels['img_path'].sample(N_SAMPLES):  # Sample for speed\n",
    "    with Image.open(img_path) as img:\n",
    "        image_shapes.append(img.size)\n",
    "\n",
    "# Create a dataframe of the images shape\n",
    "image_shapes_df = pd.DataFrame(image_shapes, columns=['Width', 'Height'])\n",
    "# Lets print it\n",
    "print(\"Images Dimensions Imformation:\\n\")\n",
    "print(image_shapes_df.describe()) # We can use the mean Width and mean Height to finetune IMG_SIZE to resize in the future\n",
    "\n",
    "# Plot image dimensions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(image_shapes_df['Width'], bins=30, color='blue', label='Width', kde=True)\n",
    "sns.histplot(image_shapes_df['Height'], bins=30, color='red', label='Height', kde=True)\n",
    "plt.title('Distribution of Images Dimensions')\n",
    "plt.xlabel('Pixels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667c603-633b-4373-bfdf-40b3b83b5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pixel_intensity(image_path):\n",
    "    '''\n",
    "    Goal: analyze image pixel intensity distribution of one image\n",
    "\n",
    "    Input: the path of an image\n",
    "\n",
    "    Outputs: -> The mean intensity of pixels in the image\n",
    "             -> The standard diviation of the pixels in the image\n",
    "    '''\n",
    "    image = Image.open(image_path)\n",
    "    image_array = np.array(image)\n",
    "    return image_array.mean(), image_array.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e0f80-f614-4fba-889f-b2fd8a87e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get analize the pixel_intensity of 100 random images\n",
    "pixel_intensity_stats = [analyze_pixel_intensity(img_path) for img_path in labels['img_path'].sample(N_SAMPLES)]\n",
    "# Lets create a dataframe for it\n",
    "pixel_intensity_df = pd.DataFrame(pixel_intensity_stats, columns=['Mean Intensity', 'Intensity Std'])\n",
    "# Lets print it\n",
    "print(pixel_intensity_df.describe())\n",
    "\n",
    "# Plot the distribution of image mean pixel intensity\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(pixel_intensity_df['Mean Intensity'], bins=30, color='blue', kde=True)\n",
    "plt.title('Distribution of Image Mean Pixel Intensity')\n",
    "plt.xlabel('Mean Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of image pixel intensity standard deviation\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(pixel_intensity_df['Intensity Std'], bins=30, color='red', kde=True)\n",
    "plt.title('Distribution of Image Intensity Standard Deviation')\n",
    "plt.xlabel('Intensity Std')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# As we can see, both look like that their folling a normal distribuition which is postive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea6554-bbee-4d47-bf27-18694b51c434",
   "metadata": {},
   "source": [
    "# Working/Processing Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12512241-eaff-4ca3-b2b6-31b4e0ebfe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_df, val_df = train_test_split(labels, test_size=0.2, stratify=labels['breed_encoded'], random_state=42)\n",
    "\n",
    "# First, we'll use the labels dataframe to split the dataset in training and validation.\n",
    "# We can do this beacuse we have the images path in the labels dataframe.\n",
    "# We will use a 20% split, that is, 80% of the data will be used to training and 20% for validation\n",
    "# The random state allows us to test always in the same data split that is, all same images go to train and alll same images go to validation\n",
    "#no matter the times we runn the code\n",
    "# We use stratify in realtion to labels_df['breed_encoded'] to make a split so that the proportion of values in the sample produced will \n",
    "#be the same as the proportion of values. Ecample. If 25% of the data is 0's and 75% is 1's, each split will have 25% 0's and 75% 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d893fed-73f2-4a53-b98b-80daf4516719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, label):\n",
    "    '''\n",
    "    Goal of function: pre-process images for training\n",
    "    \n",
    "    Inputs: -> image_path in the disk\n",
    "            -> the label associated with the image\n",
    "            \n",
    "    Outputs: -> the image pre-processed\n",
    "             -> the corresponding label of the pre-processed image\n",
    "    '''\n",
    "    \n",
    "    # Read the image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Decode the image\n",
    "    image = tf.image.decode_jpeg(image, channels=N_CHANNELS)\n",
    "    # Resize the image\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    # Scale the image to [0, 1]\n",
    "    image = image / 255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9835941-6080-4d40-932c-0a536312975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, label):\n",
    "    '''\n",
    "    Goal of function: Apply data augmentation so our training set hcan help \n",
    "                     improve the robustness and generalization of your model by creating variations of the training data.\n",
    "\n",
    "    Inputs: -> an image\n",
    "            -> the image label\n",
    "\n",
    "    Output: -> an image where all augmentation were appplyd\n",
    "            -> the image label\n",
    "    '''\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    image = tf.image.random_saturation(image, lower=0.9, upper=1.1)\n",
    "    return image, label\n",
    "\n",
    "# NOTE: The transformations (e.g., flips, brightness adjustments) are applied randomly each time an image is fetched from the dataset. \n",
    "#      Therefore, every epoch could potentially see a slightly different version of the same image.\n",
    "# It is good because: -> there is no additional storage cost.\n",
    "#                     ->  Each epoch, and even each batch, may see different augmentations, effectively increasing the diversity of the training data.\n",
    "#                     -> Models trained with augmented data are generally more robust and better at generalizing to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c231c7e-06f5-47a5-98d6-667db867f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets for the training dataframe; from_tensor_slices to convert from pandas dataframe to tf dataset.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['img_path'], train_df['breed_encoded']))\n",
    "\n",
    "# We are using TensorFlow datasets instead of Pandas dataframe beacause:\n",
    "# Performance and Efficiency:\n",
    "# -> Efficient Data Loading: TensorFlow Datasets can efficiently load, preprocess, and batch the data on the fly. \n",
    "#                            This reduces memory usage and leverages parallelism, ensuring that data is fed to the model quickly without \n",
    "#                            being bottlenecked by I/O operations.\n",
    "# -> Prefetching: TensorFlow datasets support prefetching, which allows data to be prepared while the model is training. \n",
    "#                 This keeps the GPU/TPU utilized, minimizing idle times.\n",
    "\n",
    "# Integration with TensorFlow/Keras:\n",
    "# -> Seamless Integration: TensorFlow Datasets are designed to work seamlessly with TensorFlow and Keras. \n",
    "#                          They fit naturally into the model.fit method, supporting features like shuffling, batching, and preprocessing.\n",
    "\n",
    "# Scalability:\n",
    "# -> Handling Large Datasets: For large datasets that donât fit into memory, TensorFlow Datasets can handle data \n",
    "#                            from multiple sources (e.g., files, databases) efficiently without loading everything into memory at once.\n",
    "    \n",
    "# Data Augmentation:\n",
    "# -> On-the-fly Augmentation: TensorFlow datasets support on-the-fly data augmentation, which can be crucial for training \n",
    "#                            robust models without needing to store augmented data on disk.\n",
    "\n",
    "# Lets see the first 5 lines\n",
    "count = 0\n",
    "for img, enc_label in train_ds:\n",
    "    print(img, enc_label)\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5f8ef-cb8d-4d24-b4ce-deacac8a5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the tf dataset we can pre-process\n",
    "train_ds = train_ds.map(preprocess_image).map(augment_image).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Here, the .map applies a function to each element of the dataset. It appplies the preprocess_image fuction to every tuple\n",
    "# After the image is pre-processed we create \n",
    "# The .batch groups consecutive elements of the dataset into batches of size BATCH_SIZE. Each batch will contain (img, label) * BATCH_SIZE pairs.\n",
    "# Finaly, .prefetch ensures that while one batch of data is being processed by the model, the next batch is being prepared, reducing idle time.\n",
    "# The AUTOTUNE option automatically tunes the prefetch buffer size for optimal performance.\n",
    "\n",
    "# Lets see an example\n",
    "print(next(iter(train_ds))) # The output is a batch, that is, there are 32 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070de771-ddf1-4f04-8969-e67a0e929086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we make the exact same thing but on the validation dataframe, that is, we convert it to ts Dataset\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_df['img_path'], val_df['breed_encoded']))\n",
    "val_ds = val_ds.map(preprocess_image).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94c414-49f2-489a-ab79-3f3c84fadcf4",
   "metadata": {},
   "source": [
    "# Modeling - Using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d394ce-0eff-45b0-b678-eae0616efd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first be using the VGG16 by the Tranfer Learning mode\n",
    "VGG16_model = VGG16(input_shape=(IMG_SIZE, IMG_SIZE, N_CHANNELS), include_top=False, weights='imagenet')\n",
    "\n",
    "# Here we import the VGG16 model. This is a CNN considered to be one of the best computer vision models to date.\n",
    "#    - https://medium.com/@mygreatlearning/everything-you-need-to-know-about-vgg16-7315defb5918\n",
    "\n",
    "# When we use \"include_top=False\" we are excluding the fully connected layers and \"weights='imagenet'\" the model is pre-training on ImageNet.\n",
    "# ImageNet is an image database - https://www.image-net.org/\n",
    "# For more parameters: https://keras.io/api/applications/vgg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7598d-6af4-44dc-8ae8-4518e3c608fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets say to the model that we dont want to train the layers of VGG16 because they are alredy pre-trained\n",
    "for layer in VGG16_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb2c9f-117b-490f-a25a-54c1b1f7c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets define the fully connected part of the model\n",
    "first_model = Sequential([\n",
    "    VGG16_model,\n",
    "    Flatten(), # We flatten to prepare the output\n",
    "    Dense(256, activation='relu'), # We define a fully connected layer with 256 nodes and 'relu' activation function\n",
    "    Dropout(0.5), # We prevent overfitting by randomly define 50% of the input units will be randomly set to zero at each update during training time\n",
    "    Dense(n_breeds, activation='softmax') # Then we define the output with 'softmax' because we want a probabiliti for each breed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377ffc1-e0db-4d9f-bedb-5147336ae1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets compile the model\n",
    "first_model.compile(\n",
    "    optimizer='adam', # Lets use 'adam' as the optimizer\n",
    "    loss='sparse_categorical_crossentropy', # categorical_crossentropy is used when labels are encoded with ONE HOT ENCODE\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a989213-e2b5-41b7-b22d-af911d4d3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finaly we train the model\n",
    "history = first_model.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds,\n",
    "    epochs= N_EPOCHES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043ec8c-95b4-4509-9ad8-b6172e9fb1b9",
   "metadata": {},
   "source": [
    "# Evaluation - VGG16 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910901a-f42f-4178-b42e-a74d56b7f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Training and Validation Accuracy\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "ax[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax[0].set_title('Model Accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plotting Training and Validation Loss\n",
    "ax[1].plot(history.history['loss'], label='Train Loss')\n",
    "ax[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax[1].set_title('Model Loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d486049-fa74-4065-bf65-16a44d0bcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "val_ds = val_ds.unbatch()\n",
    "y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "val_ds = val_ds.batch(BATCH_SIZE)\n",
    "y_pred = model.predict(val_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
